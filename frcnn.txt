

1. 如果改了LibPackage的header, SolDeepLearning_CPP的header也要一併更改

2. MaskAlignLayer (MaskAlign_layer.cpp)
   計算當前proposal roi與ground turth roi交集的mask
   input : proposal roi, ground turth roi, ground turth mat
   output: proposal mat
   交集已經在FrcnnProposalTargetLayer做了,可以直接從這裡拿
   (不好拿 因為Angle用top.size()>5判斷要不要做 加了Mask也有可能size==6 Layer本身不看name 無法確切知道case是哪種)  
   
   bottom: "rois" (from FrcnnProposalTargetLayer)
   bottom: "gt_boxes" (from FrcnnRoiData)
   bottom: "gt_mask" (from FrcnnRoiData)
   bottom: "labels" (generate from FrcnnProposalTargetLayer) (why need this)
   top: "masks" (to SigmoidCrossEntropyLossLayer)
   
   rois與labels是配好的 與GT的所有ROI找出最近的當作是配對好的要拿來算交集mask的
   確認該rois的labels>0表示是FG前景,則存至top[0]往LossFunc傳
   
3. ROIAlignLayer (roi_align_layer.cpp)
   雙線性插值, 優化並取代roi pooling的兩次量化的失真
   將每個proposal roi從feature map中轉化成固定width height
   的pooling計算
   
   bottom: "conv5"
   bottom: "rois"
   top: "pool5"
   
   bottom: "conv5"
   bottom: "rois"
   top: "pool6"
   
4. FrcnnProposalTargetLayer (frcnn_proposal_target_layer.cpp)
   計算當前proposal roi與所有ground turth的最大overlap配對
   
   bottom: "rpn_rois"
   bottom: "gt_boxes"
   top: "rois"
   top: "labels" (to AccuracyLayer && MaskAlignLayer)
   top: "bbox_targets"
   top: "bbox_inside_weights"
   top: "bbox_outside_weights" 
   
5. FrcnnAnchorTargetLayer (frcnn_anchor_target_layer.cpp)
   top[0] = label (1 or 0 or -1) [1,1,9*512,512]
   label.size()=config_n_anchors_*height*width=9*512*512(anchor總數)
   height=con_5_height, width=con5_width (original image size / 16) (feature map size)
   
   rpn_cls_score->top[0] [1,18,512,512]
   rpn_cls_score_reshape->top[0] [1,2,9*512,512]
   
   bottom: 'rpn_cls_score' (from ReShape)
   bottom: 'gt_boxes' (from FrcnnRoiData)
   bottom: 'im_info' (from FrcnnRoiData)
   top: 'rpn_labels' 
   top: 'rpn_bbox_targets' 
   top: 'rpn_bbox_inside_weights' 
   top: 'rpn_bbox_outside_weights' 
   
   
6. FrcnnProposalLayer (frcnn_proposal_layer)
   
   1)generate all anchors 9*512*512
   2)update new bbox using Point4f<Dtype> cbox = bbox_transform_inv(anchor, box_delta);
   3)rpn_pre_nms_top_n: 以score取前120000個anchor
     放進sort_vector,而anchors不動
   4)rpn_post_nms_top_n: 用get_iou,從score最高開始比
     刪除兩者iou大於rpn_nms_thresh且score較低者
	 比如說score第一的跟後面所有人比,iou大於0.7就消除score小的
     比過的被留下來的不會再被比
     比過被刪除的不會再被比	 
     放進box_final,scores_
	 
7. prep_im_for_blob(im, cfg.PIXEL_MEANS, target_size, cfg.TRAIN.MAX_SIZE) (https://hk.saowen.com/a/434b90f5a46f3c1a0382f03a9c138397da6dc18ad609f276a394ba09a4e84a7a)  
   其中target_size就是該圖像對應的scale=600
   這個函數先將圖像減均值，然後嘗試用target_size/短邊長度，得到一個scale，
   再用這個scale乘長邊，如果大於MAX_SIZE，則scale為MAX_SIZE/長邊長度。再使用opencv將圖像resize。
   最後得到的圖像結果是：
   要麼長邊=1000，短邊小於600；
   要麼短邊=600，長邊<=1000。

   
8. anchor (https://blog.csdn.net/XZZPPP/article/details/52317863)
   1)請問三種尺度{128,256,512}對應的原圖還是卷積後的特徵圖啊？
     對應原圖
   2)更改base_size = 16為其他值時，除了此處有變動，faster rcnn還有其他對應的地方需要更改變動嗎？
	 在程序中看來，base_size是一個基類模板大小（也可以理解為anchor），在此基礎上乘上scales，得到需要的尺寸，如128，256，512。
	 所以base_size發生變化，scales需要相應變化。
   3)將base_size變大和變小，分別適用於哪些情況？
     一般原圖如果比較大，base_size需要大點，如果原圖比較小，base_size相應需要小點。
	 
   (https://blog.csdn.net/u012905422/article/details/52634208)

   生成的9個anchor，前三排基本除去一些隨機抖動以外不同scale但是ratio相同，
   均為[-2, -1, 2, 1]，中間三排為[-1, -1, 1, 1]，最後三排為[-1, -2, 1, 2]。


	
9. RPN parameter (https://blog.csdn.net/happyflyy/article/details/54917514)
   RPN所需要的參數。其中值得注意的參數有
   batch_size：[256]每幅圖像中篩選使用的bg樣本和fg樣本的總個數
   fg_fraction：[0.5]batch_size中fg樣本的比例，如果fg樣本個數不足，則添加bg樣本
   drop_boxes_runoff_image：[1]在訓練階段是否去掉超出圖像邊界的anchors 
   bg_thresh_hi：[0.3]被看做反例樣本的anchor與groundtruth的最大IoU 
   bg_thresh_lo：[0]被看做反例樣本的anchor與groundtruth的最小IoU 
   fg_thresh ：[0.7]被看做正例樣本的anchor與groundtruth的最小IoU 
   ims_per_batch：[1]訓練時每次輸入的圖像個數，當前只支持每次輸入一幅圖像
   scale：[600]短邊縮放後最小值
   max_size：[1000]長邊縮放後最大值
   feat_stride：[16]VGG中conv5_3相比於輸入圖像縮小了16倍，也就是相鄰兩個點之間的stride=16 
   anchors：不同長寬比和尺度的9個基本anchors 
   output_width_map：輸入圖像的寬度和conv5_3寬度的對應關係
   output_height_map：輸入圖像的高度和conv5_3高度的對應關 系
   bg_weight：[1]計算損失時每個反例樣本的權值，正例樣本權值全為1 
   image_means：圖像均值	 
	 
	



	
100. mutable_cpu_data (https://blog.csdn.net/xizero00/article/details/51001206)
     mutable在這裡的含義是“易變的”。
     cpu_data()和mutable_cpu_data()這兩個函數可以理解為一個為const成員函數另一個非const。
     mutable_cpu_data()取得的數據指針意味著其數據接下來將被改變，所以要將head_置為HEAD_AT_CPU。
     這個標誌位意味著最新的數據塊在CPU內存中，而不是GPU顯存中。
     假設接下來要調用gpu_data()或mutable_gpu_data()，那麼將會從內存拷貝一份最新的數據到顯存，實現內存到顯存的數據同步。
     相反的情況同理。
     (https://blog.csdn.net/xizero00/article/details/51842704)
     當你想讀取數據的時候請使用cpu_data
     當然想修改數據的時候請你使用mutable_cpu_data。
     這樣就提示系統數據我改過啦，你要小心了的意思(我只知道數據一定在CPU上)。
   
   

101. RPN與Fast R-CNN特徵共享 (http://www.360doc.com/content/17/0303/14/10408243_633634497.shtml)
FasterRCNN算法由兩大模塊組成
1)RPN候選框提取模塊
2)FastRCNN檢測模塊
需要一種允許兩個網路間共享卷積層的技術,而不是分別學習兩個網路
FastRCNN訓練依賴於固定的目標建議框,不清楚當同時改變建議機制時,學習FastRCNN會不會收斂
RPN和FastRCNN共用了13個VGG卷積層,顯然將這兩個網路完全孤立訓練不是明智的選擇
作者採用交替訓練(Alternating Traning)階段卷積層特徵共享
Step1: 依上述訓練RPN,該網路用ImageNet預訓練的模型初始化,並端到端微調用於區域建議任務
Step2: 利用Step1的RPN生成的建議框,由FastRCNN訓練一個單獨的檢測網路,
       這個檢測網路同樣由ImageNet預訓練的模型初始化,這時候兩個網路還沒共享卷積層
Step3: 用檢測網路初始化RPN訓練,固定共享卷積層,只微調RPN獨有的層,構成一個統一的網路
Step4: 保持共享的卷積層固定,微調FastRCNN的FC層,這樣兩個網路共享相同卷積層,構成一個統一的網路



   